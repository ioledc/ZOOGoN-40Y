---
title: "LTER-MareChiara Data Processing Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{LTER-MareChiara Data Processing Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(ZooGoN)
library(dplyr)
library(readxl)
library(janitor)
library(lubridate)
library(tidyr)
```

# Overview

This vignette demonstrates the complete data processing workflow for the LTER-MareChiara zooplankton dataset, covering 40 years of monitoring (1984-2024) in the Gulf of Naples. The workflow transforms raw abundance data into Darwin Core-compliant format for integration with EMODnet Biology and the European Digital Twin of the Ocean.

## Dataset Background

The LTER-MareChiara station (40°81'N, 14°25'E) has been monitoring zooplankton communities since 1984 as part of the Long-Term Ecological Research network. This represents one of the longest continuous time series in the Mediterranean Sea.

**Key Dataset Characteristics:**
- **Temporal Coverage**: 1984-2024 (40 years)
- **Total Samples**: 1,506 
- **Taxonomic Diversity**: 148 copepod species + 61 other taxa
- **Sampling Method**: Vertical tows (0-50m depth)
- **Mesh Size**: 200 μm
- **Location**: Gulf of Naples, Tyrrhenian Sea, Western Mediterranean

# Data Processing Steps

## Comprehensive Workflow with `process_lter_data()`

The ZooGoN package provides a complete, integrated function that handles the entire processing pipeline from raw Excel files to Darwin Core format:

```{r main-function}
# Process LTER-MareChiara dataset with full workflow
processed_data <- process_lter_data(
  zoo_data_path = "data/lter_zoo_84_13.xlsx",
  ids_data_path = "data/ids.xlsx",
  worms_validation = TRUE,
  output_format = "list",
  verbose = TRUE
)

# Access the Darwin Core formatted results
event_table <- processed_data$event
occurrence_table <- processed_data$occurrence  
emof_table <- processed_data$emof
processing_info <- processed_data$processing_info
```

This single function performs all the following steps automatically:

### 1. Data Loading and Validation
- Reads Excel files with proper encoding and name repair
- Validates file paths and parameters
- Handles Excel date format conversion

### 2. Taxonomic Standardization  
- Applies `extract_genus_species()` for name standardization
- Splits genus-species names into separate columns
- Creates taxa suitable for WoRMS validation

### 3. WoRMS Taxonomic Validation (Optional)
```{r worms-validation}
# Enable WoRMS validation for taxonomic accuracy
processed_with_worms <- process_lter_data(
  worms_validation = TRUE,
  verbose = TRUE
)

# WoRMS validation provides:
# - Taxonomic authorities and publication dates  
# - LSIDs (Life Sciences Identifiers)
# - Accepted names and synonymy information
# - Classification hierarchy verification
```

### 4. Darwin Core Formatting
The function automatically creates three core Darwin Core extensions:

- **Event Extension**: Sampling events with temporal and spatial metadata
- **Occurrence Extension**: Species occurrence records with life stage information  
- **eMoF Extension**: Quantitative measurements (abundance as ind/m³)

### 5. Geographic Metadata Integration
```{r geographic-data}
# LTER-MareChiara coordinates are automatically assigned:
# - Latitude: 40.81°N
# - Longitude: 14.25°E  
# - Location: Gulf of Naples, Tyrrhenian Sea
# - Sampling protocol: Vertical tow 0-50m depth
```

## Alternative Processing Options

### Fast Processing (Skip WoRMS)
```{r fast-processing}
# Skip WoRMS validation for faster execution
quick_data <- process_lter_data(
  worms_validation = FALSE,
  verbose = TRUE
)
```

### Direct CSV Export
```{r csv-export}
# Export directly to CSV files
process_lter_data(
  output_format = "csv",
  output_dir = "processed_data/darwin_core",
  verbose = TRUE
)

# This creates:
# - event.csv (sampling events)
# - occurrence.csv (species occurrences)  
# - emof.csv (measurements)
# - raw_processed.csv (intermediate data)
# - processing_metadata.csv (workflow metadata)
```

## Manual Step-by-Step Processing

For users who prefer granular control, the individual processing steps can be performed manually:

### 1. Basic Data Loading
```{r manual-loading}
# Load zooplankton abundance data
zooplankton_data <- readxl::read_xlsx(
  "data/lter_zoo_84_13.xlsx",
  .name_repair = "minimal"
) %>%
  dplyr::mutate(dat_id = seq_len(dplyr::n()))

# Load sample identification data  
sample_ids <- readxl::read_xlsx(
  "data/ids.xlsx",
  .name_repair = "minimal"  
) %>%
  dplyr::mutate(
    date = lubridate::as_date(as.numeric(date), origin = "1899-12-30"),
    sample_id = janitor::make_clean_names(sample_id)
  )
```

### 2. Taxonomic Processing
```{r manual-taxonomy}
# Extract and standardize taxonomic information
raw_taxa <- zooplankton_data %>%
  janitor::clean_names() %>%
  dplyr::select(phylum_o_subphylum:stage, dat_id) %>%
  dplyr::mutate(
    genus_species = extract_genus_species(taxa)$genus_species
  ) %>%
  tidyr::separate(
    genus_species, 
    into = c("genus", "species"), 
    sep = " ", 
    remove = FALSE, 
    fill = "right"
  )
```

### 3. Data Restructuring
```{r manual-restructuring}
# Convert from wide to long format
dates <- zooplankton_data %>%
  dplyr::select(-c(1:10), dat_id)

tidy_data <- raw_taxa %>%
  dplyr::left_join(dates, by = "dat_id") %>%
  tidyr::pivot_longer(
    -c(dat_id, taxa, genus_species, stage),
    names_to = "date",
    values_to = "ind_m3"
  ) %>%
  dplyr::mutate(
    date = lubridate::as_date(as.numeric(date), origin = "1899-12-30")
  ) %>%
  dplyr::left_join(sample_ids, by = "date")
```

# Quality Control and Validation

## Sample Size Verification

```{r quality-control}
# Verify sample counts
cat("Total samples processed:", n_distinct(darwin_core_data$eventID), "\n")
cat("Date range:", min(darwin_core_data$eventDate), "to", max(darwin_core_data$eventDate), "\n")
cat("Unique taxa:", n_distinct(darwin_core_data$scientificName), "\n")
cat("Total occurrences:", nrow(occurrence_extension), "\n")

# Check for data completeness
missing_coords <- event_extension %>% 
  filter(is.na(decimalLatitude) | is.na(decimalLongitude))
cat("Events missing coordinates:", nrow(missing_coords), "\n")

# Verify taxonomic standardization success
standardization_summary <- occurrence_extension %>%
  summarise(
    original_taxa = n_distinct(scientificName),
    standardized_taxa = n_distinct(genus_species),
    .groups = "drop"
  )
print(standardization_summary)
```

## Data Export

Export the processed data in Darwin Core Archive format:

```{r export}
# Create output directory
output_dir <- "processed_data/darwin_core"
dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)

# Export Darwin Core tables
readr::write_csv(event_extension, 
                 file.path(output_dir, "event.csv"))
readr::write_csv(occurrence_extension, 
                 file.path(output_dir, "occurrence.csv"))  
readr::write_csv(emof_extension, 
                 file.path(output_dir, "emof.csv"))

# Export metadata
readr::write_csv(
  tibble::tibble(
    dataset_title = "40 years of Zooplankton data at LTER MareChiara site (Gulf of Naples, Mediterranean Sea) 1984-2024",
    contact = "Dr. Iole Di Capua (iole.dicapua@szn.it)",
    institution = "Stazione Zoologica Anton Dohrn",
    license = "CC-BY-NC",
    project = "DTO-BioFlow FSTP Grant",
    processing_date = Sys.Date()
  ),
  file.path(output_dir, "metadata.csv")
)
```

# Integration with EMODnet Biology

The processed Darwin Core data is ready for integration with EMODnet Biology following these standards:

## Taxonomic Validation

```{r worms-integration, eval=FALSE}
# The data will be validated against WoRMS (World Register of Marine Species)
# to obtain Life Sciences Identifiers (LSIDs) using the Taxon Match tool
# This ensures taxonomic consistency with international databases

# Example WoRMS integration workflow:
# 1. Submit genus_species names to WoRMS Taxon Match
# 2. Retrieve LSIDs and taxonomic hierarchy
# 3. Add scientificNameID column with WoRMS LSIDs
# 4. Validate against accepted names and synonyms
```

## Measurement Standardization  

```{r bodc-vocab, eval=FALSE}
# Measurements will be standardized using BODC NERC Vocabulary Server
# to ensure consistent terminology for:
# - Life stages (adult, juvenile, copepodite, etc.)
# - Measurement units (individuals per cubic meter)
# - Sampling instruments (WP2 net, Indian Ocean net)
# - Sample preservation methods (ethanol, formaldehyde)

# This enables interoperability with other marine biodiversity datasets
```

# Conclusion

This workflow transforms 40 years of LTER-MareChiara zooplankton data into a FAIR-compliant, Darwin Core-formatted dataset suitable for:

- **EMODnet Biology** publication and quality control
- **European Digital Twin of the Ocean** integration
- **International biodiversity databases** compatibility
- **Long-term ecological research** and climate change studies

The standardized dataset contributes to the EU Mission "Restore our Ocean & Waters by 2030" by providing essential biodiversity monitoring data for the Mediterranean Sea ecosystem.